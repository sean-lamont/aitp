{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch_geometric import compile"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as gloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# import models.graph_transformers.SAT.sat.models\n",
    "# import models.graph_transformers.SAT.sat.layers\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torch_geometric.data import Data\n",
    "import pickle\n",
    "import json\n",
    "from data.hol4.ast_def import *\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with open(\"data/hol4/data/torch_graph_dict.pk\", \"rb\") as f:\n",
    "#     torch_graph_dict = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with open(\"data/hol4/data/train_test_data.pk\", \"rb\") as f:\n",
    "#     train, val, test, enc_nodes = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with open(\"data/hol4/data/adjusted_db.json\") as f:\n",
    "#     db = json.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokens = list(\n",
    "#     set([token.value for polished_goal in db.keys() for token in polished_to_tokens_2(polished_goal)]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# def build_vocab(l):\n",
    "#     for token in l:\n",
    "#         yield [token]\n",
    "#\n",
    "# vocab = build_vocab_from_iterator(build_vocab(tokens), specials=[\"<UNK>\"], min_freq=0)\n",
    "# vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_seq = []\n",
    "#\n",
    "# max_len = 1024\n",
    "#\n",
    "#\n",
    "# for i, (goal, premise, y) in enumerate(train):\n",
    "#     train_seq.append(([i.value for i in polished_to_tokens_2(goal)], [i.value for i in polished_to_tokens_2(premise)], y))\n",
    "#\n",
    "# val_seq = []\n",
    "# for i, (goal, premise, y) in enumerate(val):\n",
    "#     val_seq.append(([i.value for i in polished_to_tokens_2(goal)], [i.value for i in polished_to_tokens_2(premise)], y))\n",
    "#\n",
    "# test_seq = []\n",
    "# for i, (goal, premise, y) in enumerate(test):\n",
    "#     test_seq.append(([i.value for i in polished_to_tokens_2(goal)], [i.value for i in polished_to_tokens_2(premise)], y))\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# train_goals = []\n",
    "# train_premises = []\n",
    "# train_targets = []\n",
    "#\n",
    "# for goal, premise, y in train_seq:\n",
    "#     train_goals.append(goal)\n",
    "#     train_premises.append(premise)\n",
    "#     train_targets.append(y)\n",
    "#\n",
    "#\n",
    "# val_goals = []\n",
    "# val_premises = []\n",
    "# val_targets = []\n",
    "#\n",
    "# for goal, premise, y in val_seq:\n",
    "#     val_goals.append(goal)\n",
    "#     val_premises.append(premise)\n",
    "#     val_targets.append(y)\n",
    "#\n",
    "# test_goals = []\n",
    "# test_premises = []\n",
    "# test_targets = []\n",
    "#\n",
    "# for goal, premise, y in test_seq:\n",
    "#     test_goals.append(goal)\n",
    "#     test_premises.append(premise)\n",
    "#     test_targets.append(y)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vectorise(goal_list, premise_list, target_list, max_len=1024):\n",
    "    idx_list = [vocab(toks) for toks in goal_list]\n",
    "    X_G = [sample+([0]* (max_len-len(sample))) if len(sample)<max_len else sample[:max_len] for sample in idx_list]\n",
    "    idx_list = [vocab(toks) for toks in premise_list]\n",
    "    X_P = [sample+([0]* (max_len-len(sample))) if len(sample)<max_len else sample[:max_len] for sample in idx_list]\n",
    "    return torch.tensor(X_G, dtype=torch.int32), torch.tensor(X_P, dtype=torch.int32), torch.tensor(target_list, dtype=torch.long)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_dataset = vectorise(train_goals, train_premises, train_targets)\n",
    "# val_data = vectorise(val_goals, val_premises, val_targets)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "\n",
    "        # self.initial_encoder = inner_embedding_network.F_x_module_(ntoken, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_embedding(model, input_, max_seq_len=1024):\n",
    "    out = model(input_)#, src_mask)\n",
    "    out = torch.transpose(out,1,2)\n",
    "    gmp = nn.MaxPool1d(max_seq_len, stride=1)\n",
    "    return gmp(out).squeeze(-1)#orch.cat([gmp(out).squeeze(-1), torch.sum(out,dim=2)], dim = 1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models import inner_embedding_network\n",
    "def binary_loss(preds, targets):\n",
    "    return -1. * torch.sum(targets * torch.log(preds) + (1 - targets) * torch.log((1. - preds)))\n",
    "\n",
    "\n",
    "#run_edges(1e-3, 0, 20, 1024, 64, 0, False)\n",
    "#run_2(1e-3, 0, 20, 1024, 64, 4, False)\n",
    "\n",
    "def accuracy_transformer(model_1, model_2,batch, fc):\n",
    "    g,p,y = batch\n",
    "    batch_size = len(g)\n",
    "\n",
    "    embedding_1 = gen_embedding(model_1, g.to(device))\n",
    "    embedding_2 = gen_embedding(model_2, p.to(device))\n",
    "\n",
    "    preds = fc(torch.cat([embedding_1, embedding_2], axis=1))\n",
    "\n",
    "    preds = torch.flatten(preds)\n",
    "\n",
    "    preds = (preds>0.5).long()\n",
    "\n",
    "    return torch.sum(preds == torch.LongTensor(y).to(device)) / len(y)\n",
    "\n",
    "def run_transformer_pretrain(step_size, decay_rate, num_epochs, batch_size, embedding_dim, save=False):\n",
    "\n",
    "    # loader = DataLoader(new_train, batch_size=batch_size, follow_batch=['x_s', 'x_t'])\n",
    "\n",
    "    # val_loader = iter(DataLoader(new_val, batch_size=2048, follow_batch=['x_s', 'x_t']))\n",
    "\n",
    "    G,P,Y = train_dataset\n",
    "\n",
    "    dataset = TensorDataset(G,P,Y)\n",
    "    # batch_size = 50\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    V_G, V_P, V_Y = val_data\n",
    "    val_dataset = TensorDataset(V_G, V_P, V_Y)\n",
    "\n",
    "    val_loader = iter(DataLoader(val_dataset, batch_size=batch_size))\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_1 = TransformerEmbedding(ntoken=len(vocab), d_model=embedding_dim, nhead=1, d_hid=embedding_dim, nlayers=1).to(device)\n",
    "    model_2 = TransformerEmbedding(ntoken=len(vocab), d_model=embedding_dim, nhead=1, d_hid=embedding_dim, nlayers=1).to(device)\n",
    "\n",
    "    fc = inner_embedding_network.F_c_module_(embedding_dim * 2).to(device)\n",
    "\n",
    "    op_1 =torch.optim.Adam(model_1.parameters(), lr=step_size)\n",
    "    op_2 =torch.optim.Adam(model_2.parameters(), lr=step_size)\n",
    "    op_fc =torch.optim.Adam(fc.parameters(), lr=step_size)\n",
    "\n",
    "    training_losses = []\n",
    "\n",
    "    val_losses = []\n",
    "    best_acc = 0.\n",
    "\n",
    "\n",
    "    for j in range(num_epochs):\n",
    "        print (f\"Epoch: {j}\")\n",
    "\n",
    "        for batch_idx, (g,p,y) in enumerate(loader):\n",
    "            # op_enc.zero_grad()\n",
    "            op_1.zero_grad()\n",
    "            op_2.zero_grad()\n",
    "            op_fc.zero_grad()\n",
    "\n",
    "            embedding_1 = gen_embedding(model_1, g.to(device))\n",
    "            embedding_2 = gen_embedding(model_2, p.to(device))\n",
    "\n",
    "            preds = fc(torch.cat([embedding_1, embedding_2], axis=1))\n",
    "\n",
    "            eps = 1e-6\n",
    "\n",
    "            preds = torch.clip(preds, eps, 1 - eps)\n",
    "\n",
    "            loss = binary_loss(torch.flatten(preds), torch.LongTensor(y).to(device))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            op_1.step()\n",
    "            op_2.step()\n",
    "            op_fc.step()\n",
    "\n",
    "            training_losses.append(loss.detach() / batch_size)\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "\n",
    "                validation_loss = accuracy_transformer(model_1, model_2, next(val_loader), fc)#, fp, fi, fo, fx, fc,conv1,conv2, graph_iterations)\n",
    "\n",
    "                val_losses.append((validation_loss.detach(), j, batch_idx))\n",
    "\n",
    "                val_loader = iter(DataLoader(val_dataset, batch_size=batch_size))\n",
    "\n",
    "                print (\"Curr training loss avg: {}\".format(sum(training_losses[-100:]) / len(training_losses[-100:])))\n",
    "\n",
    "                print (\"Val acc: {}\".format(validation_loss.detach()))\n",
    "\n",
    "    return training_losses, val_losses\n",
    "\n",
    "# run_transformer_pretrain(1e-3, 0, 40, 32, 128, 2)#, save=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# G,P,Y = train_dataset\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_goals[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Vectorise sequence without requiring a maximum length\n",
    "'''\n",
    "def to_mp_data(data_list, vocab):\n",
    "    return [Data(torch.tensor(vocab(toks), dtype=torch.int32)) for toks in data_list]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# len(train_goals[])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_graphs = to_mp_data(train_goals, vocab)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch_geometric.nn as gnn\n",
    "from einops import rearrange, repeat\n",
    "import torch_geometric.utils as utils\n",
    "\n",
    "'''\n",
    "Implementation of standard transformer through message passing. Generates a fully connected graph on input sequence\n",
    "and performs self attention using message passing.\n",
    "\n",
    "Batching is done through PyG, with a batch consisting only of (batch_size, d_model),\n",
    "as opposed to standard (batch_size, max_seq_len, d_model)\n",
    "'''\n",
    "\n",
    "def ptr_to_complete_edge_index(ptr):\n",
    "    # print (ptr)\n",
    "    from_lists = [torch.arange(ptr[i], ptr[i + 1]).repeat_interleave(ptr[i + 1] - ptr[i]) for i in range(len(ptr) - 1)]\n",
    "    to_lists = [torch.arange(ptr[i], ptr[i + 1]).repeat(ptr[i + 1] - ptr[i]) for i in range(len(ptr) - 1)]\n",
    "    combined_complete_edge_index = torch.vstack((torch.cat(from_lists, dim=0), torch.cat(to_lists, dim=0)))\n",
    "    return combined_complete_edge_index\n",
    "\n",
    "\n",
    "class MPAttention(gnn.MessagePassing):\n",
    "\n",
    "\n",
    "    def __init__(self, embed_dim,  num_heads=8, dropout=0., bias=False, **kwargs):\n",
    "\n",
    "        super().__init__(node_dim=0, aggr='add')\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.bias = bias\n",
    "\n",
    "        head_dim = embed_dim // num_heads\n",
    "\n",
    "        # print (embed_dim, num_heads, head_dim)\n",
    "\n",
    "\n",
    "        assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "\n",
    "        self.to_qk = nn.Linear(embed_dim, embed_dim * 2, bias=bias)\n",
    "\n",
    "        self.to_v = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        # self.ffn = torch.nn.Sequential(nn.Linear(embed_dim, embed_dim * 4, bias=bias),\n",
    "        #                                nn.ReLU(),\n",
    "        #                                nn.Linear(embed_dim * 4, embed_dim * 2))\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.attn_sum = None\n",
    "\n",
    "        # print (f\"Attn network {self}\")\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "        nn.init.xavier_uniform_(self.to_qk.weight)\n",
    "\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.to_qk.weight)\n",
    "            nn.init.constant_(self.to_v.bias, 0.)\n",
    "\n",
    "    def forward(self,\n",
    "                x,\n",
    "                edge_index,\n",
    "                complete_edge_index,\n",
    "                edge_attr=None,\n",
    "                ptr=None,\n",
    "                return_attn=False):\n",
    "\n",
    "        assert ptr is not None\n",
    "\n",
    "        # if edge_index is None:\n",
    "        #     edge_index = ptr_to_complete_edge_index(ptr.cpu()).cuda()\n",
    "\n",
    "\n",
    "\n",
    "        qk = self.to_qk(x).chunk(2, dim=-1)\n",
    "\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        attn = None\n",
    "\n",
    "\n",
    "        out = self.propagate(complete_edge_index, v=v, qk=qk, edge_attr=None, size=None,\n",
    "                             return_attn=return_attn)\n",
    "\n",
    "        # print (out.shape)\n",
    "\n",
    "        out = rearrange(out, 'n h d -> n (h d)')\n",
    "\n",
    "\n",
    "        # if return_attn:\n",
    "        #     attn = self._attn\n",
    "        #     self._attn = None\n",
    "        #     attn = torch.sparse_coo_tensor(\n",
    "        #         complete_edge_index,\n",
    "        #         attn,\n",
    "        #     ).to_dense().transpose(0, 1)\n",
    "\n",
    "\n",
    "        return self.out_proj(out), attn\n",
    "\n",
    "    def message(self, v_j, qk_j, qk_i, edge_attr, index, ptr, size_i, return_attn):\n",
    "        \"\"\"Self-attention operation compute the dot-product attention \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # print (f\"v_j {v_j.shape}, qk_j: {qk_j.shape}, qk_i: {qk_i.shape}\")\n",
    "        #\n",
    "        # print (f\"index {index.shape}\\n\\n\")\n",
    "\n",
    "        #todo AMR make sure size_i isn't breaking softmax for non-complete index\n",
    "\n",
    "        # size_i = max(index) + 1 # from torch_geometric docs? todo test correct\n",
    "\n",
    "        # qk_j is keys i.e. message \"from\" j, qk_i maps to queries i.e. messages \"to\" i\n",
    "\n",
    "        # index maps to the \"to\"/ i values i.e. index[i] = 3 means i = 3, and len(index) is the number of messages\n",
    "        # i.e. index will be 0,n repeating n times (if complete_edge_index is every combination of nodes)\n",
    "\n",
    "\n",
    "        qk_i = rearrange(qk_i, 'n (h d) -> n h d', h=self.num_heads)\n",
    "        qk_j = rearrange(qk_j, 'n (h d) -> n h d', h=self.num_heads)\n",
    "        v_j = rearrange(v_j, 'n (h d) -> n h d', h=self.num_heads)\n",
    "\n",
    "        # print (f\"post arrange v_j {v_j.shape}, qk_j: {qk_j.shape}, qk_i: {qk_i.shape}\")\n",
    "        # sum over dimension, giving n h shape\n",
    "        attn = (qk_i * qk_j).sum(-1) * self.scale\n",
    "\n",
    "        # print (f\"attn shape {attn.shape}\")\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            attn = attn + edge_attr\n",
    "\n",
    "        # index gives what to softmax over\n",
    "\n",
    "        attn = utils.softmax(attn, index, ptr, size_i)\n",
    "\n",
    "        # print (f\"attn shape after softmax {attn.shape}\")\n",
    "\n",
    "        if return_attn:\n",
    "            self._attn = attn\n",
    "\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        msg = v_j * attn.unsqueeze(-1)\n",
    "\n",
    "        # print (f\"msg shape {msg.shape}\")\n",
    "\n",
    "        return msg\n",
    "\n",
    "class MPTransformerEncoderLayer(nn.TransformerEncoderLayer):\n",
    "\n",
    "    def __init__(self, d_model, nhead=8, dim_feedforward=512, dropout=0.1,\n",
    "                 activation=\"relu\", batch_norm=True, pre_norm=False,\n",
    "                 **kwargs):\n",
    "\n",
    "\n",
    "        # print (nhead)\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "\n",
    "        self.self_attn = MPAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout,\n",
    "                                     bias=False, **kwargs)\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "        if batch_norm:\n",
    "            self.norm1 = nn.BatchNorm1d(d_model)\n",
    "            self.norm2 = nn.BatchNorm1d(d_model)\n",
    "\n",
    "    def forward(self, x, edge_index,complete_edge_index,\n",
    "                ptr=None,\n",
    "                return_attn=False,\n",
    "                ):\n",
    "\n",
    "        if self.pre_norm:\n",
    "            x = self.norm1(x)\n",
    "\n",
    "        x2, attn = self.self_attn(\n",
    "            x,\n",
    "            edge_index,\n",
    "            complete_edge_index,\n",
    "            ptr=ptr,\n",
    "            return_attn=return_attn\n",
    "        )\n",
    "\n",
    "        x = x + self.dropout1(x2)\n",
    "\n",
    "        if self.pre_norm:\n",
    "            x = self.norm2(x)\n",
    "        else:\n",
    "            x = self.norm1(x)\n",
    "\n",
    "        x2 = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "        x = x + self.dropout2(x2)\n",
    "\n",
    "        if not self.pre_norm:\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MPPositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1024):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x, ptr):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        pe_ptr = torch.cat([self.pe[:(ptr[i+1] - ptr[i])] for i in range(len(ptr) - 1)], dim = 0)\n",
    "\n",
    "        # print (pe_ptr.shape)\n",
    "\n",
    "        return pe_ptr\n",
    "\n",
    "class MPTransformerEncoder(nn.TransformerEncoder):\n",
    "\n",
    "    def forward(self, x, edge_index, complete_edge_index, edge_attr=None,ptr=None, return_attn=False):\n",
    "\n",
    "        output = x\n",
    "\n",
    "        for mod in self.layers:\n",
    "\n",
    "            output = mod(output,\n",
    "                         edge_index=edge_index,\n",
    "                         complete_edge_index=complete_edge_index,\n",
    "                         ptr=ptr,\n",
    "                         return_attn=return_attn\n",
    "                         )\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class MPTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, d_model, num_heads=4,\n",
    "                 dim_feedforward=512, dropout=0.2, num_layers=2,\n",
    "                 batch_norm=False, pe=False,\n",
    "                 in_embed=True, use_global_pool=True, max_seq_len=None,\n",
    "                 global_pool='mean', **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # print (f\"insize {in_size}, d_model {d_model}, num_heads: {num_heads}\")\n",
    "        # self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=256)\n",
    "\n",
    "        # if pos_encoder:\n",
    "        #     self.pos_encoder = MPPositionalEncoding(d_model, dropout)\n",
    "        # else:\n",
    "        #     self.pos_encoder = None\n",
    "\n",
    "        self.pe = pe\n",
    "\n",
    "        if in_embed:\n",
    "            if isinstance(in_size, int):\n",
    "                self.embedding = nn.Embedding(in_size, d_model)\n",
    "            else:\n",
    "                raise ValueError(\"Not implemented!\")\n",
    "        else:\n",
    "            self.embedding = nn.Linear(in_features=in_size,\n",
    "                                       out_features=d_model,\n",
    "                                       bias=False)\n",
    "\n",
    "\n",
    "        encoder_layer = MPTransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_norm=batch_norm,**kwargs)\n",
    "\n",
    "        self.encoder = MPTransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.global_pool = global_pool\n",
    "\n",
    "        if global_pool == 'mean':\n",
    "            self.pooling = gnn.global_mean_pool\n",
    "\n",
    "        elif global_pool == 'add':\n",
    "            self.pooling = gnn.global_add_pool\n",
    "\n",
    "        elif global_pool == 'cls':\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, d_model))\n",
    "            self.pooling = None\n",
    "\n",
    "        self.use_global_pool = use_global_pool\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, data, return_attn=False):\n",
    "\n",
    "        output = data.x\n",
    "\n",
    "        ptr = data.ptr\n",
    "\n",
    "        complete_edge_index = data.complete_edge_index\n",
    "\n",
    "        # if hasattr(data, 'edge_index'):\n",
    "        #     edge_index = data.edge_index\n",
    "        # else:\n",
    "        #     edge_index = ptr_to_complete_edge_index(ptr.cpu()).cuda()\n",
    "\n",
    "\n",
    "        output = self.embedding(output)\n",
    "\n",
    "\n",
    "        if self.pe:\n",
    "            # print (data.pe[0], output[0], data.pe.shape, output.shape)\n",
    "            output = output + data.pe\n",
    "\n",
    "\n",
    "        if self.global_pool == 'cls' and self.use_global_pool:\n",
    "            bsz = len(data.ptr) - 1\n",
    "\n",
    "            # if edge_index is not None:\n",
    "            #     new_index = torch.vstack((torch.arange(data.num_nodes).to(data.batch), data.batch + data.num_nodes))\n",
    "            #     new_index2 = torch.vstack((new_index[1], new_index[0]))\n",
    "            #     idx_tmp = torch.arange(data.num_nodes, data.num_nodes + bsz).to(data.batch)\n",
    "            #     new_index3 = torch.vstack((idx_tmp, idx_tmp))\n",
    "            #     edge_index = torch.cat((\n",
    "            #         edge_index, new_index, new_index2, new_index3), dim=-1)\n",
    "\n",
    "            degree = None\n",
    "\n",
    "            cls_tokens = repeat(self.cls_token, '() d -> b d', b=bsz)\n",
    "\n",
    "            output = torch.cat((output, cls_tokens))\n",
    "\n",
    "        output = self.encoder(\n",
    "            output,\n",
    "            edge_index = None,\n",
    "            complete_edge_index=complete_edge_index,\n",
    "            ptr=data.ptr,\n",
    "            return_attn=return_attn\n",
    "        )\n",
    "\n",
    "        if self.use_global_pool:\n",
    "\n",
    "            if self.global_pool == 'cls':\n",
    "                output = output[-bsz:]\n",
    "\n",
    "            else:\n",
    "                # output_1 = self.pooling(output, data.batch)\n",
    "                output = gnn.global_max_pool(output, data.batch)\n",
    "                # output = torch.cat([output_1, output_2], dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "'''\n",
    "Transformer encoder for nested tensor input, requires no padding\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "class NestedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., kdim=None, vdim=None,bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if kdim is None and vdim is None:\n",
    "            self.kdim = embed_dim\n",
    "            self.vdim = embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Linear(embed_dim, self.embed_dim)\n",
    "\n",
    "        self.k = nn.Linear(embed_dim, self.kdim)\n",
    "        self.v = nn.Linear(embed_dim, self.vdim)\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        # self._reset_parameters()\n",
    "\n",
    "        self.attn_sum = None\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # Q,K,V shape (batch, n, dim)\n",
    "\n",
    "        Q = self.q(q)\n",
    "        K = self.k(k)\n",
    "        V = self.v(v)\n",
    "\n",
    "        # print (Q.size(0), Q[0].shape, K.size(0), K[0].shape)\n",
    "\n",
    "        # split into (batch, n, num_heads, dim)\n",
    "\n",
    "        Q = Q.reshape(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.reshape(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.reshape(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn = torch.matmul(Q, K.transpose(-1,-2)) * self.scale\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        if self.dropout > 0.0:\n",
    "           attn = F.dropout(attn, p=self.dropout)\n",
    "\n",
    "        out = torch.matmul(attn, V)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NestedTransformerEncoderLayer(nn.TransformerEncoderLayer):\n",
    "\n",
    "    def __init__(self, d_model, nhead=8, dim_feedforward=512, dropout=0.1,\n",
    "                 activation=\"relu\", layer_norm=False, pre_norm=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "\n",
    "        # self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead,dropout=dropout, batch_first=True)\n",
    "        self.self_attn = NestedAttention(embed_dim=d_model, num_heads=nhead,dropout=dropout)\n",
    "\n",
    "        self.layer_norm = layer_norm\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "        if layer_norm:\n",
    "            self.norm1 = nn.LayerNorm(d_model)#(d_model)\n",
    "            self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "\n",
    "        if self.pre_norm:\n",
    "            x = self.norm1(x)\n",
    "\n",
    "        # x2, attn = self.self_attn(x,x,x)\n",
    "        x2 = self.self_attn(x,x,x)\n",
    "\n",
    "        x = x + self.dropout1(x2)\n",
    "\n",
    "        if self.pre_norm:\n",
    "            x = self.norm2(x)\n",
    "            x = self.norm1(x)\n",
    "\n",
    "        x2 = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "        x = x + self.dropout2(x2)\n",
    "\n",
    "        # if not self.pre_norm:\n",
    "        #     x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class NestedPositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1024):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x, ptr):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        pe_ptr = torch.cat([self.pe[:(ptr[i+1] - ptr[i])] for i in range(len(ptr) - 1)], dim = 0)\n",
    "\n",
    "        # print (pe_ptr.shape)\n",
    "\n",
    "        return pe_ptr\n",
    "\n",
    "class NestedTransformerEncoder(nn.TransformerEncoder):\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "\n",
    "        output = x\n",
    "\n",
    "        for mod in self.layers:\n",
    "\n",
    "            output = mod(output, return_attn=return_attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class NestedTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, d_model, num_heads=4,\n",
    "                 dim_feedforward=512, dropout=0.2, num_layers=2,\n",
    "                 layer_norm=False, pe=False,\n",
    "                 in_embed=True, use_global_pool=True,\n",
    "                 global_pool='mean', **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.pe = pe\n",
    "\n",
    "        if in_embed:\n",
    "            if isinstance(in_size, int):\n",
    "                self.embedding = nn.Embedding(in_size, d_model)\n",
    "            else:\n",
    "                raise ValueError(\"Not implemented!\")\n",
    "        else:\n",
    "            self.embedding = nn.Linear(in_features=d_model,\n",
    "                                       out_features=d_model,\n",
    "                                       bias=False)\n",
    "\n",
    "\n",
    "        encoder_layer = NestedTransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, layer_norm=layer_norm,**kwargs)\n",
    "\n",
    "        self.encoder = NestedTransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.global_pool = global_pool\n",
    "\n",
    "        if global_pool == 'mean':\n",
    "            self.pooling = gnn.global_mean_pool\n",
    "\n",
    "        elif global_pool == 'add':\n",
    "            self.pooling = gnn.global_add_pool\n",
    "\n",
    "        elif global_pool == 'cls':\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, d_model))\n",
    "            self.pooling = None\n",
    "\n",
    "        self.use_global_pool = use_global_pool\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "\n",
    "        output = self.embedding(x)\n",
    "\n",
    "\n",
    "        # if self.pe:\n",
    "        # print (data.pe[0], output[0], data.pe.shape, output.shape)\n",
    "        # output = output +\n",
    "\n",
    "\n",
    "        if self.global_pool == 'cls' and self.use_global_pool:\n",
    "            bsz = x.shape(0)\n",
    "\n",
    "            # if edge_index is not None:\n",
    "            #     new_index = torch.vstack((torch.arange(data.num_nodes).to(data.batch), data.batch + data.num_nodes))\n",
    "            #     new_index2 = torch.vstack((new_index[1], new_index[0]))\n",
    "            #     idx_tmp = torch.arange(data.num_nodes, data.num_nodes + bsz).to(data.batch)\n",
    "            #     new_index3 = torch.vstack((idx_tmp, idx_tmp))\n",
    "            #     edge_index = torch.cat((\n",
    "            #         edge_index, new_index, new_index2, new_index3), dim=-1)\n",
    "\n",
    "            degree = None\n",
    "\n",
    "            cls_tokens = repeat(self.cls_token, '() d -> b d', b=bsz)\n",
    "\n",
    "            output = torch.cat((output, cls_tokens))\n",
    "\n",
    "        output = self.encoder(\n",
    "            output,\n",
    "            return_attn=return_attn\n",
    "        )\n",
    "\n",
    "        if self.use_global_pool:\n",
    "\n",
    "            if self.global_pool == 'cls':\n",
    "                output = output[-bsz:]\n",
    "\n",
    "            else:\n",
    "                # output_1 = self.pooling(output, data.batch)\n",
    "                # output = gnn.global_max_pool(output, data.batch)\n",
    "                # output = gnn.global_max_pool(output, data.batch)\n",
    "                # output = torch.cat([output_1, output_2], dim=1)\n",
    "\n",
    "                output = output.unbind()\n",
    "                # sum pool\n",
    "                output = [torch.sum(output[i], dim=0) for i in range(len(output))]\n",
    "                output = torch.stack(output, dim=0)\n",
    "        return  output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from time import time\n",
    "from utils.viz_net_torch import make_dot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with open(\"mp_transformer_test_old_new.pk\", \"wb\") as f:\n",
    "#     pickle.dump((G, train_graphs), f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with open(\"mp_transformer_test_old_new.pk\", \"rb\") as f:\n",
    "#     G, train_graphs = pickle.load(f)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# start = time()\n",
    "#\n",
    "# loader = iter(gloader(train_graphs, batch_size=3))\n",
    "# batch = next(loader)\n",
    "# batch.complete_edge_index = ptr_to_complete_edge_index(batch.ptr)\n",
    "# pe = MPPositionalEncoding(128)\n",
    "# batch.pe = pe(batch.x, batch.ptr).squeeze(1)\n",
    "#\n",
    "# print (f\"Data load time {time() - start}\")\n",
    "#\n",
    "#\n",
    "# model = MPTransformer(in_size=len(vocab), d_model=128, dim_feedforward=128, num_layers=1,num_heads=1,in_embed=True,dropout=0.,max_seq_len=None,batch_norm=False,pe=True, global_pool='max').to(device)\n",
    "#\n",
    "# print (f\"Model define time {time() - start}\")\n",
    "# start = time()\n",
    "#\n",
    "# embedding = model(batch.cuda())\n",
    "#\n",
    "# print (f\"Model run time {time() - start}\")\n",
    "#\n",
    "#\n",
    "# start = time()\n",
    "#\n",
    "# loss = torch.sum(embedding)\n",
    "#\n",
    "# g = make_dot(loss)\n",
    "# g.view()\n",
    "#\n",
    "# loss.backward()\n",
    "#\n",
    "# print (f\"Model backward time: {time() - start}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mem 4644, 1.19 time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# start = time()\n",
    "#\n",
    "# old_loader = iter(DataLoader(G, batch_size=32))\n",
    "# old_batch = next(old_loader)\n",
    "#\n",
    "# print (f\"Data load time {time() - start}\")\n",
    "#\n",
    "# start = time()\n",
    "#\n",
    "# tf_model = TransformerEmbedding(ntoken=len(vocab), d_model=128, nhead=1, d_hid=128, nlayers=1).to(device)\n",
    "#\n",
    "# print (f\"Model define time {time() - start}\")\n",
    "#\n",
    "# start = time()\n",
    "# embedding_1 = gen_embedding(tf_model, old_batch.to(device))\n",
    "#\n",
    "#\n",
    "# print (f\"Model run time {time() - start}\")\n",
    "#\n",
    "# start = time()\n",
    "#\n",
    "# loss = torch.sum(embedding_1)\n",
    "#\n",
    "#\n",
    "# g = make_dot(loss)\n",
    "# g.view()\n",
    "#\n",
    "# loss.backward()\n",
    "#\n",
    "# finish = time() - start\n",
    "#\n",
    "# print (f\"Model backward time: {time() - start}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#mem 7800 time 1.9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def review_preprocess(review):\n",
    "    \"\"\"\n",
    "    Takes in a string of review, then performs the following:\n",
    "    1. Remove HTML tag from review\n",
    "    2. Remove URLs from review\n",
    "    3. Make entire review lowercase\n",
    "    4. Split the review in words\n",
    "    5. Remove all punctuation\n",
    "    6. Remove empty strings from review\n",
    "    7. Remove all stopwords\n",
    "    8. Returns a list of the cleaned review after jioning them back to a sentence\n",
    "    \"\"\"\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "\n",
    "    \"\"\"\n",
    "    Removing HTML tag from review\n",
    "    \"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    review_without_tag = re.sub(clean, '', review)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Removing URLs\n",
    "    \"\"\"\n",
    "    review_without_tag_and_url = re.sub(r\"http\\S+\", \"\", review_without_tag)\n",
    "\n",
    "    review_without_tag_and_url = re.sub(r\"www\\S+\", \"\", review_without_tag)\n",
    "\n",
    "    \"\"\"\n",
    "    Make entire string lowercase\n",
    "    \"\"\"\n",
    "    review_lowercase = review_without_tag_and_url.lower()\n",
    "\n",
    "    \"\"\"\n",
    "    Split string into words\n",
    "    \"\"\"\n",
    "    list_of_words = word_tokenize(review_lowercase)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Remove punctuation\n",
    "    Checking characters to see if they are in punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_words_without_punctuation=[''.join(this_char for this_char in this_string if (this_char in string.ascii_lowercase))for this_string in list_of_words]\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Remove empty strings\n",
    "    \"\"\"\n",
    "    list_of_words_without_punctuation = list(filter(None, list_of_words_without_punctuation))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Remove any stopwords\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_word_list = [w for w in list_of_words_without_punctuation if w not in en_stops]\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a list of the cleaned review after jioning them back to a sentence\n",
    "    \"\"\"\n",
    "    return ' '.join(filtered_word_list)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load file into memory\n",
    "\"\"\"\n",
    "def load_file(filename):\n",
    "    \"\"\"\n",
    "    Open the file as read only\n",
    "    \"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    \"\"\"\n",
    "    Read all text\n",
    "    \"\"\"\n",
    "    text = file.read()\n",
    "    \"\"\"\n",
    "    Close the file\n",
    "    \"\"\"\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def get_data(directory, vocab, is_trian):\n",
    "    \"\"\"\n",
    "    Reading train test directory\n",
    "    \"\"\"\n",
    "    review_dict={'neg':[],'pos':[]}\n",
    "    if is_trian:\n",
    "        directory = os.path.join(directory+'/train')\n",
    "    else:\n",
    "        directory = os.path.join(directory+'/test')\n",
    "    print('Directory : ',directory)\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        data_folder=os.path.join(directory, label_type)\n",
    "        print('Data Folder : ',data_folder)\n",
    "        for root, dirs, files in os.walk(data_folder):\n",
    "            for fname in files:\n",
    "                if fname.endswith(\".txt\"):\n",
    "                    file_name_with_full_path=os.path.join(root, fname)\n",
    "                    review=load_file(file_name_with_full_path)\n",
    "                    clean_review=review_preprocess(review)\n",
    "                    if label_type == 'neg':\n",
    "                        review_dict['neg'].append(clean_review)\n",
    "                    else:\n",
    "                        review_dict['pos'].append(clean_review)\n",
    "                    \"\"\"\n",
    "                    Update counts\n",
    "                    \"\"\"\n",
    "                    vocab.update(clean_review.split())\n",
    "\n",
    "    return review_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import re\n",
    "import string"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "vocab = Counter()\n",
    "directory='/home/sean/Downloads/aclImdb_v1/aclImdb'\n",
    "\n",
    "try:\n",
    "    with open(\"imbd_dicts\", \"rb\") as f:\n",
    "        train_review_dict, test_review_dict, word_list, vocab_to_int, int_to_vocab = pickle.load(f)\n",
    "except:\n",
    "    train_review_dict=get_data(directory, vocab, True)\n",
    "    test_review_dict=get_data(directory, vocab, False)\n",
    "\n",
    "    word_list = sorted(vocab, key = vocab.get, reverse = True)\n",
    "    vocab_to_int = {word:idx+1 for idx, word in enumerate(word_list)}\n",
    "    int_to_vocab = {idx:word for word, idx in vocab_to_int.items()}\n",
    "\n",
    "    with open(\"imbd_dicts\", \"wb\") as f:\n",
    "        pickle.dump((train_review_dict, test_review_dict, word_list, vocab_to_int, int_to_vocab), f)\n",
    "\n",
    "total_time=time.time()-startTime\n",
    "\n",
    "print('Time Taken : ',total_time/60,'minutes')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab = vocab_to_int"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Number of negative reviews in train set :',len(train_review_dict['neg']))\n",
    "print('Number of positive reviews in train set :',len(train_review_dict['pos']))\n",
    "\n",
    "print('\\nNumber of negative reviews in test set :',len(test_review_dict['neg']))\n",
    "print('Number of positive reviews in test set :',len(test_review_dict['pos']))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class IMDBReviewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, review_dict, alphabet):\n",
    "\n",
    "\n",
    "        self.data = review_dict\n",
    "        self.labels = [x for x in review_dict.keys()]\n",
    "        self.alphabet = alphabet\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(x) for x in self.data.values()])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = 0\n",
    "        while idx >= len(self.data[self.labels[label]]):\n",
    "            idx -= len(self.data[self.labels[label]])\n",
    "            label += 1\n",
    "        reviewText = self.data[self.labels[label]][idx]\n",
    "\n",
    "\n",
    "\n",
    "        label_vec = torch.zeros((1), dtype=torch.long)\n",
    "        label_vec[0] = label\n",
    "        return self.reviewText2InputVec(reviewText), label\n",
    "\n",
    "    def reviewText2InputVec(self, review_text):\n",
    "        T = len(review_text)\n",
    "\n",
    "        review_text_vec = torch.zeros((T), dtype=torch.long)\n",
    "        encoded_review=[]\n",
    "        for pos,word in enumerate(review_text.split()):\n",
    "            if word not in vocab_to_int.keys():\n",
    "                \"\"\"\n",
    "                If word is not available in vocab_to_int dict puting 0 in that place\n",
    "                \"\"\"\n",
    "                review_text_vec[pos]=0\n",
    "            else:\n",
    "                review_text_vec[pos]=vocab_to_int[word]\n",
    "\n",
    "        return review_text_vec\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pad_and_pack(batch):\n",
    "    input_tensors = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for x, y in batch:\n",
    "        input_tensors.append(x)\n",
    "        labels.append(y)\n",
    "        lengths.append(x.shape[0]) #Assume shape is (T, *)\n",
    "\n",
    "    longest = max(lengths)\n",
    "    print (longest)\n",
    "    print (sum(lengths) / len(lengths))\n",
    "    #We need to pad all the inputs up to 'longest', and combine into a batch ourselves\n",
    "    if len(input_tensors[0].shape) == 1:\n",
    "        x_padded = torch.nn.utils.rnn.pad_sequence(input_tensors, batch_first=False)\n",
    "    else:\n",
    "        raise Exception('Current implementation only supports (T) shaped data')\n",
    "\n",
    "    x_packed = torch.nn.utils.rnn.pack_padded_sequence(x_padded, lengths, batch_first=False, enforce_sorted=False)\n",
    "\n",
    "    y_batched = torch.as_tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return x_packed, y_batched"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B = 24\n",
    "train_dataset=IMDBReviewDataset(train_review_dict,vocab)\n",
    "test_dataset=IMDBReviewDataset(test_review_dict,vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=pad_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_and_pack)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for random batch, 3000 max len, 900 avg. So large padding difference\n",
    "# batch = next(iter(train_loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 831 avg length, 9100 max, so large difference\n",
    "# sum([len(train_dataset[i][0]) for i in range(len(train_dataset))]) / len(train_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_dataset[0][0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [int_to_vocab[i] for i in train_dataset[0][0].tolist() if i != 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# len(vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_dataset[0][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vectorise_imdb(data, max_len = 1024):\n",
    "    X_G = [sample+([0]* (max_len-len(sample))) if len(sample)<max_len else sample[:max_len] for sample in data]\n",
    "    return torch.tensor(X_G, dtype=torch.int32)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_train = []\n",
    "for x, y in train_dataset:\n",
    "    x = [i for i in x.tolist() if i != 0]\n",
    "    new_train.append((x,y))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# max([len(new_train[i][0]) for i in range(len(new_train))])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.hist([len(new_train[i][0]) for i in range(len(new_train))])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_pyg_data(data_list):\n",
    "    return [Data(x=torch.LongTensor(x[0]), y=torch.tensor(x[1])) if len(x[0]) < 1024 else Data(x=torch.LongTensor(x[0][:1024]), y=torch.tensor(x[1])) for x in data_list ]\n",
    "\n",
    "\n",
    "train_graphs = to_pyg_data(new_train)\n",
    "\n",
    "\n",
    "def to_tensor_list(data_list):\n",
    "    return [(torch.LongTensor(x[0]),torch.tensor(x[1])) if len(x[0]) < 1024 else (torch.LongTensor(x[0][:1024]), torch.tensor(x[1])) for x in data_list ]\n",
    "\n",
    "\n",
    "tensor_list = to_tensor_list(new_train)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# graph_loader = iter(gloader(final_graphs, batch_size=batch_size))\n",
    "# batch = next(loader)\n",
    "# batch.complete_edge_index = ptr_to_complete_edge_index(batch.ptr)\n",
    "# pe = MPPositionalEncoding(embedding_dim)\n",
    "# batch.pe = pe(batch.x, batch.ptr).squeeze(1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec_train = vectorise_imdb([new_train[i][0] for i in range(len(new_train))])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# new_train[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# vec_train[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# assert len(new_train) == len(vec_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_train = [(vec_train[i], new_train[i][1]) for i in range(len(new_train))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# new_train[0][0][:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# final_train[0][0][:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loader = DataLoader(final_train, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch = next(iter(loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models import inner_embedding_network\n",
    "\n",
    "def binary_loss(preds, targets):\n",
    "    return -1. * torch.sum(targets * torch.log(preds) + (1 - targets) * torch.log((1. - preds)))\n",
    "\n",
    "\n",
    "def accuracy_transformer(x,y,model,fc):\n",
    "\n",
    "    embedding = gen_embedding(model, x.to(device))\n",
    "\n",
    "    preds = fc(x)\n",
    "\n",
    "    preds = torch.flatten(preds)\n",
    "\n",
    "    preds = (preds>0.5).long()\n",
    "\n",
    "    return torch.sum(preds == torch.LongTensor(y).to(device)) / len(y)\n",
    "\n",
    "def run_transformer_pretrain(step_size, decay_rate, num_epochs, batch_size, embedding_dim, save=False):\n",
    "\n",
    "    # loader = DataLoader(new_train, batch_size=batch_size, follow_batch=['x_s', 'x_t'])\n",
    "\n",
    "    # val_loader = iter(DataLoader(new_val, batch_size=2048, follow_batch=['x_s', 'x_t']))\n",
    "\n",
    "    loader = DataLoader(final_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    # val_loader = iter(DataLoader(val_dataset, batch_size=batch_size))\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = \"cpu\"\n",
    "\n",
    "    model = TransformerEmbedding(ntoken=len(vocab), d_model=embedding_dim, nhead=1, d_hid=embedding_dim, nlayers=1).to(device)\n",
    "\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    fc = inner_embedding_network.F_c_module_(embedding_dim).to(device)\n",
    "\n",
    "    op_1 =torch.optim.Adam(model.parameters(), lr=step_size)\n",
    "\n",
    "    op_fc =torch.optim.Adam(fc.parameters(), lr=step_size)\n",
    "\n",
    "    training_losses = []\n",
    "\n",
    "    # val_losses = []\n",
    "    best_acc = 0.\n",
    "\n",
    "\n",
    "    for j in range(num_epochs):\n",
    "        print (f\"Epoch: {j}\")\n",
    "\n",
    "        start = time.time()\n",
    "        for batch_idx, (x,y) in enumerate(loader):\n",
    "            # op_enc.zero_grad()\n",
    "            op_1.zero_grad()\n",
    "            op_fc.zero_grad()\n",
    "\n",
    "            embedding = gen_embedding(model, x.to(device))\n",
    "\n",
    "            preds = fc(embedding)\n",
    "\n",
    "            eps = 1e-6\n",
    "\n",
    "            preds = torch.clip(preds, eps, 1 - eps)\n",
    "\n",
    "            loss = binary_loss(torch.flatten(preds), torch.LongTensor(y).to(device))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            op_1.step()\n",
    "            op_fc.step()\n",
    "\n",
    "            training_losses.append(loss.detach() / batch_size)\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "\n",
    "                # validation_loss = accuracy_transformer(model_1, model_2, next(val_loader), fc)#, fp, fi, fo, fx, fc,conv1,conv2, graph_iterations)\n",
    "                #\n",
    "                # val_losses.append((validation_loss.detach(), j, batch_idx))\n",
    "                #\n",
    "                # val_loader = iter(DataLoader(val_dataset, batch_size=batch_size))\n",
    "\n",
    "                print (\"Curr training loss avg: {}\".format(sum(training_losses[-100:]) / len(training_losses[-100:])))\n",
    "\n",
    "                # print (\"Val acc: {}\".format(validation_loss.detach()))\n",
    "\n",
    "        print (f\"Time for epoch {time.time() - start}\")\n",
    "    return #training_losses\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run_transformer_pretrain(1e-4, 0, 5, 32, 128)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def accuracy_mp_transformer(x,y,model,fc):\n",
    "\n",
    "    embedding = gen_embedding(model, x.to(device))\n",
    "\n",
    "    preds = fc(x)\n",
    "\n",
    "    preds = torch.flatten(preds)\n",
    "\n",
    "    preds = (preds>0.5).long()\n",
    "\n",
    "    return torch.sum(preds == torch.LongTensor(y).to(device)) / len(y)\n",
    "\n",
    "def run_mp_transformer_pretrain(step_size, decay_rate, num_epochs, batch_size, embedding_dim):\n",
    "\n",
    "    # loader = DataLoader(new_train, batch_size=batch_size, follow_batch=['x_s', 'x_t'])\n",
    "\n",
    "    # val_loader = iter(DataLoader(new_val, batch_size=2048, follow_batch=['x_s', 'x_t']))\n",
    "\n",
    "    # loader = DataLoader(final_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    graph_loader = gloader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # val_loader = iter(DataLoader(val_dataset, batch_size=batch_size))\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = \"cpu\"\n",
    "\n",
    "    model = MPTransformer(in_size=len(vocab), d_model=embedding_dim, dim_feedforward=embedding_dim, num_layers=1,num_heads=1,in_embed=True,dropout=0.,max_seq_len=None,batch_norm=False,pe=False, global_pool='max').to(device)\n",
    "\n",
    "    # model = torch.compile(model)\n",
    "\n",
    "    # model = compile(model)\n",
    "\n",
    "    fc = inner_embedding_network.F_c_module_(embedding_dim).to(device)\n",
    "\n",
    "    op_1 =torch.optim.Adam(model.parameters(), lr=step_size)\n",
    "\n",
    "    op_fc =torch.optim.Adam(fc.parameters(), lr=step_size)\n",
    "\n",
    "    training_losses = []\n",
    "\n",
    "    # val_losses = []\n",
    "    best_acc = 0.\n",
    "\n",
    "    pe = MPPositionalEncoding(embedding_dim)\n",
    "\n",
    "    for j in range(num_epochs):\n",
    "        print (f\"Epoch: {j}\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(graph_loader):\n",
    "            # op_enc.zero_grad()\n",
    "            op_1.zero_grad()\n",
    "            op_fc.zero_grad()\n",
    "\n",
    "            batch.complete_edge_index = ptr_to_complete_edge_index(batch.ptr)\n",
    "            # batch.pe = pe(batch.x, batch.ptr).squeeze(1)\n",
    "\n",
    "            def embed():\n",
    "                return model(batch.to(device))\n",
    "\n",
    "            embedding = embed()\n",
    "\n",
    "            preds = fc(embedding)\n",
    "\n",
    "            eps = 1e-6\n",
    "\n",
    "            preds = torch.clip(preds, eps, 1 - eps)\n",
    "\n",
    "            loss = binary_loss(torch.flatten(preds), batch.y.to(device))\n",
    "\n",
    "\n",
    "            # g = make_dot(loss)\n",
    "            # g.show()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            op_1.step()\n",
    "            op_fc.step()\n",
    "\n",
    "            training_losses.append(loss.detach() / batch_size)\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "\n",
    "\n",
    "                # validation_loss = accuracy_transformer(model_1, model_2, next(val_loader), fc)#, fp, fi, fo, fx, fc,conv1,conv2, graph_iterations)\n",
    "                #\n",
    "                # val_losses.append((validation_loss.detach(), j, batch_idx))\n",
    "                #\n",
    "                # val_loader = iter(DataLoader(val_dataset, batch_size=batch_size))\n",
    "\n",
    "                print (\"Curr training loss avg: {}\".format(sum(training_losses[-100:]) / len(training_losses[-100:])))\n",
    "\n",
    "                # print (\"Val acc: {}\".format(validation_loss.detach()))\n",
    "\n",
    "        print (f\"Time for epoch {time.time() - start}\")\n",
    "\n",
    "    return #training_losses\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cProfile\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "\n",
    "cProfile.run('run_transformer_pretrain(1e-4, 0, 5, 32, 128)', sort='cumtime')\n",
    "# run_mp_transformer_pretrain(1e-4, 0, 5, 32, 128)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# run('run_mp_transformer_pretrain(1e-4, 0, 5, 32, 128)', sort='cumtime')\n",
    "\n",
    "\n",
    "def accuracy_nested_transformer(x,y,model,fc):\n",
    "\n",
    "    embedding = gen_embedding(model, x.to(device))\n",
    "\n",
    "    preds = fc(x)\n",
    "\n",
    "    preds = torch.flatten(preds)\n",
    "\n",
    "    preds = (preds>0.5).long()\n",
    "\n",
    "    return torch.sum(preds == torch.LongTensor(y).to(device)) / len(y)\n",
    "\n",
    "embed = nn.Embedding(len(vocab), 128)\n",
    "# embed = torch.compile(embed)\n",
    "\n",
    "def run_nested_transformer_pretrain(step_size, decay_rate, num_epochs, batch_size, embedding_dim):\n",
    "\n",
    "    # loader = DataLoader(new_train, batch_size=batch_size, follow_batch=['x_s', 'x_t'])\n",
    "\n",
    "    # val_loader = iter(DataLoader(new_val, batch_size=2048, follow_batch=['x_s', 'x_t']))\n",
    "\n",
    "    # loader = DataLoader(final_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    graph_loader = gloader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # val_loader = iter(DataLoader(val_dataset, batch_size=batch_size))\n",
    "\n",
    "    # device = \"cpu\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = NestedTransformer(in_size=len(vocab), d_model=embedding_dim, dim_feedforward=embedding_dim, num_layers=1,num_heads=1,in_embed=False,dropout=0.,max_seq_len=None,layer_norm=False,pe=False, global_pool='max').to(device)\n",
    "\n",
    "    # model = torch.compile(model)\n",
    "\n",
    "    fc = inner_embedding_network.F_c_module_(embedding_dim).to(device)\n",
    "    # fc = torch.compile(fc)\n",
    "\n",
    "    op_1 =torch.optim.Adam(model.parameters(), lr=step_size)\n",
    "\n",
    "    op_fc =torch.optim.Adam(fc.parameters(), lr=step_size)\n",
    "\n",
    "    op_enc =torch.optim.Adam(embed.parameters(), lr=step_size)\n",
    "\n",
    "    training_losses = []\n",
    "\n",
    "    # val_losses = []\n",
    "    best_acc = 0.\n",
    "\n",
    "    pe = MPPositionalEncoding(embedding_dim)\n",
    "    import random\n",
    "    random.shuffle(tensor_list)\n",
    "    for j in range(num_epochs):\n",
    "        print (f\"Epoch: {j}\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(0, len(tensor_list) // batch_size):\n",
    "\n",
    "            op_1.zero_grad()\n",
    "            op_fc.zero_grad()\n",
    "            op_enc.zero_grad()\n",
    "\n",
    "            batch = tensor_list[batch_size * i: batch_size * (i + 1)]\n",
    "\n",
    "            y = torch.stack([batch[j][1] for j in range(len(batch))])\n",
    "\n",
    "            x = [embed(batch[j][0]) for j in range(len(batch))]\n",
    "\n",
    "            # op_enc.zero_grad()\n",
    "\n",
    "            nested_in = torch.nested.nested_tensor(x)\n",
    "\n",
    "            embedding = model(nested_in.to(device))\n",
    "\n",
    "            # print (embedding.shape)\n",
    "\n",
    "            preds = fc(embedding)\n",
    "\n",
    "            eps = 1e-6\n",
    "\n",
    "            preds = torch.clip(preds, eps, 1 - eps)\n",
    "\n",
    "            loss = binary_loss(torch.flatten(preds), y.to(device))\n",
    "\n",
    "\n",
    "            # g = make_dot(loss)\n",
    "            # g.show()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            op_1.step()\n",
    "            op_fc.step()\n",
    "            op_enc.step()\n",
    "\n",
    "            training_losses.append(loss.detach() / batch_size)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "\n",
    "\n",
    "                # validation_loss = accuracy_transformer(model_1, model_2, next(val_loader), fc)#, fp, fi, fo, fx, fc,conv1,conv2, graph_iterations)\n",
    "                #\n",
    "                # val_losses.append((validation_loss.detach(), j, batch_idx))\n",
    "                #\n",
    "                # val_loader = iter(DataLoader(val_dataset, batch_size=batch_size))\n",
    "\n",
    "                print (\"Curr training loss avg: {}\".format(sum(training_losses[-100:]) / len(training_losses[-100:])))\n",
    "\n",
    "                # print (\"Val acc: {}\".format(validation_loss.detach()))\n",
    "\n",
    "        print (f\"Time for epoch {time.time() - start}\")\n",
    "\n",
    "    return #training_losses\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cProfile"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cProfile.run('run_nested_transformer_pretrain(1e-4, 0, 5, 32, 128)', sort='cumtime')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "test_vals = [2,34,6,3]\n",
    "\n",
    "tensor_list_1 = [torch.rand(i, 128) for i in test_vals]\n",
    "tensor_list_2 = [torch.rand(i, 128) for i in test_vals]\n",
    "\n",
    "nt1 = torch.nested.nested_tensor(tensor_list_1)\n",
    "nt2 = torch.nested.nested_tensor(tensor_list_2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A,B = nt1.chunk(2, dim=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A.size(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "nt1.reshape(4,-1,2,64).transpose(1, 2)[1].shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = NestedAttention(128,1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = model(nt1, nt1, nt1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "one_nt = torch.nested.as_nested_tensor([torch.ones(i).unsqueeze(1) for i in test_vals])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%timeit nest_sum=torch.matmul(out.transpose(1,2), one_nt).squeeze(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "o = out.unbind()\n",
    "o_sum = [torch.sum(o[i], dim=0) for i in range(len(o))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "o = out.unbind()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "o[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.matmul(nt1,nt2.transpose(1, 2)) * 0.25"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AttentionAMR(gnn.MessagePassing):\n",
    "    \"\"\"Multi-head AMR attention implementation using PyG interface\n",
    "\n",
    "    Args:\n",
    "    ----------\n",
    "    embed_dim (int):        the embeding dimension\n",
    "    num_heads (int):        number of attention heads (default: 8)\n",
    "    dropout (float):        dropout value (default: 0.0)\n",
    "    bias (bool):            whether layers have an additive bias (default: False)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, edge_dim = 0, num_heads=8, dropout=0., bias=False, **kwargs):\n",
    "\n",
    "        super().__init__(node_dim=0, aggr='add')\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.bias = bias\n",
    "\n",
    "        head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.r_proj = nn.Linear(embed_dim * 2 + edge_dim, embed_dim , bias=bias)\n",
    "\n",
    "        self.to_q = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.to_k = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.to_v = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.combine_source_target  = nn.Linear(embed_dim * 2, embed_dim, bias=bias)\n",
    "\n",
    "        self.ffn = torch.nn.Sequential(nn.Linear(embed_dim, embed_dim * 4, bias=bias),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(embed_dim * 4, embed_dim * 2))\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.attn_sum = None\n",
    "\n",
    "        # print (f\"Attn network {self}\")\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_k.weight)\n",
    "\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.to_q.weight)\n",
    "            nn.init.xavier_uniform_(self.to_k.weight)\n",
    "            nn.init.constant_(self.to_v.bias, 0.)\n",
    "\n",
    "    def forward(self,\n",
    "                x_source,\n",
    "                x_target,\n",
    "                edge_index,\n",
    "                edge_attr=None,\n",
    "                ptr=None,\n",
    "                return_attn=False):\n",
    "        \"\"\"\n",
    "        Compute attention layer.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        x:                          input node features\n",
    "        edge_index:                 edge index from the graph\n",
    "        complete_edge_index:        edge index from fully connected graph\n",
    "        subgraph_node_index:        documents the node index in the k-hop subgraphs\n",
    "        subgraph_edge_index:        edge index of the extracted subgraphs\n",
    "        subgraph_indicator_index:   indices to indicate to which subgraph corresponds to which node\n",
    "        subgraph_edge_attr:         edge attributes of the extracted k-hop subgraphs\n",
    "        edge_attr:                  edge attributes\n",
    "        return_attn:                return attention (default: False)\n",
    "\n",
    "        \"\"\"\n",
    "        # Compute value matrix\n",
    "\n",
    "        # print (x_source.shape)\n",
    "        # print (x_target.shape)\n",
    "        if edge_attr:\n",
    "            R = torch.stack([torch.cat([x_source[edge_index[0][i]], edge_attr[i], x_target[edge_index[1][i]]], dim = 0)  for i in range(len(edge_index[0]))], dim=1)\n",
    "        else:\n",
    "            R = torch.stack([torch.cat([x_source[edge_index[0][i]], x_target[edge_index[1][i]]], dim = 0)  for i in range(len(edge_index[0]))], dim=1)\n",
    "\n",
    "        R = rearrange(R, \"d n -> n d\")\n",
    "\n",
    "        #\"complete_edge_index\" which has \"from\" relations \"to\" source nodes, and \"from\" relations to the corresponding \"target\" nodes\n",
    "        edge_index_source = torch.LongTensor([[i for i in range(edge_index.shape[1])], [edge_index[0][i] for i in range(edge_index.shape[1])]])\n",
    "\n",
    "        edge_index_target = torch.LongTensor([[i for i in range(edge_index.shape[1])], [edge_index[1][i] for i in range(edge_index.shape[1])]])\n",
    "\n",
    "        Q_source = self.to_q(x_source)\n",
    "        Q_target = self.to_q(x_target)\n",
    "\n",
    "        R = self.r_proj(R)\n",
    "\n",
    "        V = self.to_v(R)\n",
    "        K = self.to_k(R)\n",
    "\n",
    "        attn = None\n",
    "\n",
    "        # print (f\"R : {R}, Q_source: {Q_source}, Q_target: {Q_target}, V: {V}, K: {K}\")\n",
    "\n",
    "        out_source = self.propagate(edge_index_source, v=V, qk=(K, Q_source), edge_attr=None, size=None,\n",
    "                                    return_attn=return_attn)\n",
    "\n",
    "        out_target = self.propagate(edge_index_target, v=V, qk=(K, Q_target), edge_attr=None, size=None,\n",
    "                                    return_attn=return_attn)\n",
    "\n",
    "\n",
    "        out_source = rearrange(out_source, 'n h d -> n (h d)')\n",
    "\n",
    "        out_target = rearrange(out_target, 'n h d -> n (h d)')\n",
    "\n",
    "        out_source = self.out_proj(out_source)\n",
    "\n",
    "        out_target = self.out_proj(out_target)\n",
    "\n",
    "        scale = F.sigmoid(self.combine_source_target(torch.cat([out_source, out_target], dim = 1)))\n",
    "\n",
    "        out = scale * out_source + (1 - scale) * out_target\n",
    "\n",
    "        O_source, O_target = self.ffn(out).chunk(2, dim=-1)\n",
    "\n",
    "        x_source = self.layer_norm(x_source + O_source)\n",
    "        x_target = self.layer_norm(x_target + O_target)\n",
    "\n",
    "\n",
    "        # if return_attn:\n",
    "        #     attn = self._attn\n",
    "        #     self._attn = None\n",
    "        #     attn = torch.sparse_coo_tensor(\n",
    "        #         complete_edge_index,\n",
    "        #         attn,\n",
    "        #     ).to_dense().transpose(0, 1)\n",
    "\n",
    "\n",
    "        return x_source, x_target\n",
    "\n",
    "    def message(self, v_j, qk_j, qk_i, edge_attr, index, ptr, size_i, return_attn):\n",
    "        \"\"\"Self-attention operation compute the dot-product attention \"\"\"\n",
    "\n",
    "\n",
    "        # print (f\"v_j {v_j}, qk_j: {qk_j}, qk_i: {qk_i}\")\n",
    "\n",
    "        # print (f\"index {index}\\n\\n\")\n",
    "\n",
    "        #todo AMR make sure size_i isn't breaking softmax for non-complete index\n",
    "\n",
    "        # size_i = max(index) + 1 # from torch_geometric docs? todo test correct\n",
    "\n",
    "        # qk_j is keys i.e. message \"from\" j, qk_i maps to queries i.e. messages \"to\" i\n",
    "\n",
    "        # index maps to the \"to\"/ i values i.e. index[i] = 3 means i = 3, and len(index) is the number of messages\n",
    "        # i.e. index will be 0,n repeating n times (if complete_edge_index is every combination of nodes)\n",
    "\n",
    "        # print (f\"qkj: {qk_j}, qki: {qk_i}, vj: {v_j}\")\n",
    "\n",
    "        # print (f\"message: v_j {v_j.shape}, qk_j: {qk_j.shape}, index: {index}, ptr: {ptr}, size_i: {size_i}\")\n",
    "\n",
    "        qk_i = rearrange(qk_i, 'n (h d) -> n h d', h=self.num_heads)\n",
    "        qk_j = rearrange(qk_j, 'n (h d) -> n h d', h=self.num_heads)\n",
    "        v_j = rearrange(v_j, 'n (h d) -> n h d', h=self.num_heads)\n",
    "\n",
    "        # print (f\"message after: v_j {v_j.shape}, qk_j: {qk_j.shape}, index: {index}, ptr: {ptr}, size_i: {size_i}\")\n",
    "\n",
    "        # sum over dimension, giving n h shape\n",
    "        attn = (qk_i * qk_j).sum(-1) * self.scale\n",
    "\n",
    "        # print (attn.shape)\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            attn = attn + edge_attr\n",
    "\n",
    "        # index gives what to softmax over\n",
    "\n",
    "        attn = utils.softmax(attn, index, ptr, size_i)\n",
    "        if return_attn:\n",
    "            self._attn = attn\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        return v_j * attn.unsqueeze(-1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AttentionAMR(nn.Module):\n",
    "    \"\"\"Multi-head AMR attention implementation using PyG interface\n",
    "\n",
    "    Args:\n",
    "    ----------\n",
    "    embed_dim (int):        the embeding dimension\n",
    "    num_heads (int):        number of attention heads (default: 8)\n",
    "    dropout (float):        dropout value (default: 0.0)\n",
    "    bias (bool):            whether layers have an additive bias (default: False)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, edge_dim = 0, num_heads=8, dropout=0., bias=False, **kwargs):\n",
    "\n",
    "        super().__init__(node_dim=0, aggr='add')\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.bias = bias\n",
    "\n",
    "        head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.r_proj = nn.Linear(embed_dim * 2 + edge_dim, embed_dim , bias=bias)\n",
    "\n",
    "        self.to_q = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.to_k = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.to_v = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.combine_source_target  = nn.Linear(embed_dim * 2, embed_dim, bias=bias)\n",
    "\n",
    "        self.ffn = torch.nn.Sequential(nn.Linear(embed_dim, embed_dim * 4, bias=bias),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(embed_dim * 4, embed_dim * 2))\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.attn_sum = None\n",
    "\n",
    "        # print (f\"Attn network {self}\")\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_k.weight)\n",
    "\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.to_q.weight)\n",
    "            nn.init.xavier_uniform_(self.to_k.weight)\n",
    "            nn.init.constant_(self.to_v.bias, 0.)\n",
    "\n",
    "    def forward(self,\n",
    "                x_source,\n",
    "                x_target,\n",
    "                edge_index,\n",
    "                edge_attr=None,\n",
    "                ptr=None,\n",
    "                return_attn=False):\n",
    "        \"\"\"\n",
    "        Compute attention layer.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        x:                          input node features\n",
    "        edge_index:                 edge index from the graph\n",
    "        complete_edge_index:        edge index from fully connected graph\n",
    "        subgraph_node_index:        documents the node index in the k-hop subgraphs\n",
    "        subgraph_edge_index:        edge index of the extracted subgraphs\n",
    "        subgraph_indicator_index:   indices to indicate to which subgraph corresponds to which node\n",
    "        subgraph_edge_attr:         edge attributes of the extracted k-hop subgraphs\n",
    "        edge_attr:                  edge attributes\n",
    "        return_attn:                return attention (default: False)\n",
    "\n",
    "        \"\"\"\n",
    "        # Compute value matrix\n",
    "\n",
    "        # print (x_source.shape)\n",
    "        # print (x_target.shape)\n",
    "        if edge_attr:\n",
    "            R = torch.stack([torch.cat([x_source[edge_index[0][i]], edge_attr[i], x_target[edge_index[1][i]]], dim = 0)  for i in range(len(edge_index[0]))], dim=1)\n",
    "        else:\n",
    "            R = torch.stack([torch.cat([x_source[edge_index[0][i]], x_target[edge_index[1][i]]], dim = 0)  for i in range(len(edge_index[0]))], dim=1)\n",
    "\n",
    "        R = rearrange(R, \"d n -> n d\")\n",
    "\n",
    "        #\"complete_edge_index\" which has \"from\" relations \"to\" source nodes, and \"from\" relations to the corresponding \"target\" nodes\n",
    "        edge_index_source = torch.LongTensor([[i for i in range(edge_index.shape[1])], [edge_index[0][i] for i in range(edge_index.shape[1])]])\n",
    "\n",
    "        edge_index_target = torch.LongTensor([[i for i in range(edge_index.shape[1])], [edge_index[1][i] for i in range(edge_index.shape[1])]])\n",
    "\n",
    "        Q_source = self.to_q(x_source)\n",
    "        Q_target = self.to_q(x_target)\n",
    "\n",
    "        R = self.r_proj(R)\n",
    "\n",
    "        V = self.to_v(R)\n",
    "        K = self.to_k(R)\n",
    "\n",
    "        attn = None\n",
    "\n",
    "        # print (f\"R : {R}, Q_source: {Q_source}, Q_target: {Q_target}, V: {V}, K: {K}\")\n",
    "\n",
    "        out_source = self.propagate(edge_index_source, v=V, qk=(K, Q_source), edge_attr=None, size=None,\n",
    "                                    return_attn=return_attn)\n",
    "\n",
    "        out_target = self.propagate(edge_index_target, v=V, qk=(K, Q_target), edge_attr=None, size=None,\n",
    "                                    return_attn=return_attn)\n",
    "\n",
    "\n",
    "        out_source = rearrange(out_source, 'n h d -> n (h d)')\n",
    "\n",
    "        out_target = rearrange(out_target, 'n h d -> n (h d)')\n",
    "\n",
    "        out_source = self.out_proj(out_source)\n",
    "\n",
    "        out_target = self.out_proj(out_target)\n",
    "\n",
    "        scale = F.sigmoid(self.combine_source_target(torch.cat([out_source, out_target], dim = 1)))\n",
    "\n",
    "        out = scale * out_source + (1 - scale) * out_target\n",
    "\n",
    "        O_source, O_target = self.ffn(out).chunk(2, dim=-1)\n",
    "\n",
    "        x_source = self.layer_norm(x_source + O_source)\n",
    "        x_target = self.layer_norm(x_target + O_target)\n",
    "\n",
    "\n",
    "        # if return_attn:\n",
    "        #     attn = self._attn\n",
    "        #     self._attn = None\n",
    "        #     attn = torch.sparse_coo_tensor(\n",
    "        #         complete_edge_index,\n",
    "        #         attn,\n",
    "        #     ).to_dense().transpose(0, 1)\n",
    "\n",
    "\n",
    "        return x_source, x_target\n",
    "\n",
    "    def message(self, v_j, qk_j, qk_i, edge_attr, index, ptr, size_i, return_attn):\n",
    "        \"\"\"Self-attention operation compute the dot-product attention \"\"\"\n",
    "\n",
    "\n",
    "        # print (f\"v_j {v_j}, qk_j: {qk_j}, qk_i: {qk_i}\")\n",
    "\n",
    "        # print (f\"index {index}\\n\\n\")\n",
    "\n",
    "        #todo AMR make sure size_i isn't breaking softmax for non-complete index\n",
    "\n",
    "        # size_i = max(index) + 1 # from torch_geometric docs? todo test correct\n",
    "\n",
    "        # qk_j is keys i.e. message \"from\" j, qk_i maps to queries i.e. messages \"to\" i\n",
    "\n",
    "        # index maps to the \"to\"/ i values i.e. index[i] = 3 means i = 3, and len(index) is the number of messages\n",
    "        # i.e. index will be 0,n repeating n times (if complete_edge_index is every combination of nodes)\n",
    "\n",
    "        # print (f\"qkj: {qk_j}, qki: {qk_i}, vj: {v_j}\")\n",
    "\n",
    "        # print (f\"message: v_j {v_j.shape}, qk_j: {qk_j.shape}, index: {index}, ptr: {ptr}, size_i: {size_i}\")\n",
    "\n",
    "        qk_i = rearrange(qk_i, 'n (h d) -> n h d', h=self.num_heads)\n",
    "        qk_j = rearrange(qk_j, 'n (h d) -> n h d', h=self.num_heads)\n",
    "        v_j = rearrange(v_j, 'n (h d) -> n h d', h=self.num_heads)\n",
    "\n",
    "        # print (f\"message after: v_j {v_j.shape}, qk_j: {qk_j.shape}, index: {index}, ptr: {ptr}, size_i: {size_i}\")\n",
    "\n",
    "        # sum over dimension, giving n h shape\n",
    "        attn = (qk_i * qk_j).sum(-1) * self.scale\n",
    "\n",
    "        # print (attn.shape)\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            attn = attn + edge_attr\n",
    "\n",
    "        # index gives what to softmax over\n",
    "\n",
    "        attn = utils.softmax(attn, index, ptr, size_i)\n",
    "        if return_attn:\n",
    "            self._attn = attn\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        return v_j * attn.unsqueeze(-1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch",
   "language": "python",
   "display_name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
